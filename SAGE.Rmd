---
title: "SAGE"
author: "Gregorio Ponti"
date: "`r Sys.Date()`"
output: 
  pdf_document: 
    fig_width: 7
    fig_caption: yes
    fig_crop: no
    number_sections: yes
---

```{r setup, warning = FALSE, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE)
```

# Intro

## Import Libraries

```{r, message=FALSE}
library(tidyverse)
library(tidymodels)
library(ggpubr)
library(sjPlot)
theme_set(theme_pubr())

#for statistics
library(car)
library(lme4)
library(lmerTest)

# for EFA
library(psych) #Main FA work
library(corrplot)
library(nFactors) #Help with number of factors to extract
library(FactoMineR) #Additional functions
library(parameters)
library(lavaan) #For CFA
```

## Summary

Import -\> Tidy Data -\> Transform into what we want -\> Analyze

### EFA process

1.  Calculate the Kaiser-Meyer-Olkin (KMO) values for every item. If any items have a KMO below the cutoff value, then the item with the lowest value is removed and the step is repeated. KMO values above 0.6 are kept, though above 0.8 are preferred. KMO measures the suitability for factor analysis by estimating the proportion of variance among all observed variables.
2.  Check whether the items can be factored using Bartlett's test of sphericity. A low p-score indicates that factor analysis can be performed. Compares the correlation matrix to the identity matrix (checks whether there are correlations)
3.  Calculate the EFA model using factoring and a specified number of factors.
4.  Calculate the commonalities, which are the proportion of the item's variance explained by the factors. If any item is below the cutoff (\<0.2), then the item with the lowest value is dropped and then restart at Step 1.
5.  Calculate the item loadings. If there are items that fail to load to any factor, then remove the item with the smallest max loading and then restart at Step 1.
6.  Create a model for the CFA by placing each item onto the factor that contains the item's largest loading. If any items load equally onto more than one factor, then add to all factors where this is the case.
7.  Fit this model using Confirmatory Factor Analysis to the original data and extract a fit statistic (Akaike information criterion, or similar) to be used as a comparison for the ideal number of factors.
8.  Change the number of factors and repeat the above steps.
9.  Plot the fit statistic vs the number of factors. The model with the local minimum index is the preferred model.

# Data Prep

## Import Data

```{r}
raw_df <- read.csv(file = "./ExportedFiles/SAGE_Raw.csv")
```

## Process Data

Columns 1, 2, 54, 55 (intervention) are course information Columns 3 - 34 are the questions from SAGE Columns 35 - 53 are demographics questions

```{r}
dat <- raw_df[,3:34]
set.seed(42)
df_split <- initial_split(raw_df, prop = 0.5)
train_data <- training(df_split)
test_data <- testing(df_split)
```

# EFA

## Correlations

```{r}
M = cor(dat)
corrplot(M, method = 'color', tl.pos='n')
```

## KMO Test

-   0.00 to 0.49 unacceptable
-   0.50 to 0.59 miserable
-   0.60 to 0.69 mediocre
-   0.70 to 0.79 middling
-   0.80 to 0.89 meritorious
-   0.90 to 1.00 marvelous

```{r}
KMO(dat)
dat <- dat[, KMO(dat)$MSAi>0.6]
```

## Bartlett's Test of Sphericity

```{r, message = FALSE}
cortest.bartlett(dat)
```

## Scree Plot

```{r}
ev <- eigen(M)
ev$values
scree(dat)
fa.parallel(dat, fa="fa")
```

## EFA loop

Uses the training data set

```{r, message = FALSE, warning = FALSE}
for (Nfacs in 3:9){
  df <- train_data[,3:34]
  loadings_test = TRUE
  while (loadings_test) {
    communs_test = TRUE
    while (communs_test){
      df <- df[, KMO(df)$MSAi>0.6]
      if (cortest.bartlett(df)$p.value > 0.05){print("Bartlett test failed")}
      df.efa <- fa(df, nfactors = Nfacs, rotate = "oblimin")
      if (min(abs(df.efa$communality))<0.2) {
        df <- df[, -c(which.min(abs(df.efa$communality)))]
      }
      else {
        communs_test = FALSE
      }
    }
    cutoff <- 0.1
    Lambda <- unclass(df.efa$loadings)
    p <- nrow(Lambda)
    fx <- setNames(Lambda, NULL)
    fx[abs(Lambda) < cutoff] <- NA_real_
    fx <- as.data.frame(fx)
    fx$max <- do.call(pmax, c(abs(fx), na.rm = TRUE))
    if (min(fx$max)<0.3) { 
      df <- df[, -which.min(fx$max)]
    }  
    else {
      loadings_test = FALSE
      }
  }
  str <- paste0("cfa.fit",Nfacs)
  model <- efa_to_cfa(df.efa)
  cfa.fit <- cfa(model = model, data = test_data)
  assign(str,cfa.fit)
}

```

## CFA

Now we need to compare the CFA outputs of each and determine which is the optimal model

```{r}
anova(cfa.fit3, cfa.fit4, cfa.fit5, cfa.fit6, cfa.fit7, cfa.fit8, cfa.fit9)
```

From this, both 7 and 9 factors are near equivalent with n=7 to have a lower AIC and n=9 to have a lower Chisq.

```{r}
df.efa <- fa(dat, nfactors = 7, rotate = "oblimin")
df.efa$loadings
```
